@inproceedings{long2018pde,
  title={Pde-net: Learning pdes from data},
  author={Long, Zichao and Lu, Yiping and Ma, Xianzhong and Dong, Bin},
  booktitle={International Conference on Machine Learning},
  pages={3208--3216},
  year={2018},
  organization={PMLR}
}


@article{long2019pde,
  title={PDE-Net 2.0: Learning PDEs from data with a numeric-symbolic hybrid deep network},
  author={Long, Zichao and Lu, Yiping and Dong, Bin},
  journal={Journal of Computational Physics},
  volume={399},
  pages={108925},
  year={2019},
  publisher={Elsevier}
}


@article{SIRIGNANO20181339,
title = {DGM: A deep learning algorithm for solving partial differential equations},
journal = {Journal of Computational Physics},
volume = {375},
pages = {1339-1364},
year = {2018},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.08.029},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118305527},
author = {Justin Sirignano and Konstantinos Spiliopoulos},
keywords = {Partial differential equations, Machine learning, Deep learning, High-dimensional partial differential equations},
abstract = {High-dimensional PDEs have been a longstanding computational challenge. We propose to solve high-dimensional PDEs by approximating the solution with a deep neural network which is trained to satisfy the differential operator, initial condition, and boundary conditions. Our algorithm is meshfree, which is key since meshes become infeasible in higher dimensions. Instead of forming a mesh, the neural network is trained on batches of randomly sampled time and space points. The algorithm is tested on a class of high-dimensional free boundary PDEs, which we are able to accurately solve in up to 200 dimensions. The algorithm is also tested on a high-dimensional Hamilton–Jacobi–Bellman PDE and Burgers' equation. The deep learning algorithm approximates the general solution to the Burgers' equation for a continuum of different boundary conditions and physical conditions (which can be viewed as a high-dimensional space). We call the algorithm a “Deep Galerkin Method (DGM)” since it is similar in spirit to Galerkin methods, with the solution approximated by a neural network instead of a linear combination of basis functions. In addition, we prove a theorem regarding the approximation power of neural networks for a class of quasilinear parabolic PDEs.}
}

@article{RAISSI2019686,
title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
journal = {Journal of Computational Physics},
volume = {378},
pages = {686-707},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
author = {M. Raissi and P. Perdikaris and G.E. Karniadakis}
}


@inproceedings{wang1990structured,
  title={Structured trainable networks for matrix algebra},
  author={Wang, Lixin and Mendel, Jerry M},
  booktitle={1990 IJCNN International Joint Conference on Neural Networks},
  pages={125--132},
  year={1990},
  organization={IEEE}
}

@inproceedings{gobovic1993design,
  title={Design of locally connected CMOS neural cells to solve the steady-state heat flow problem},
  author={Gobovic, D and Zaghloul, ME},
  booktitle={Proceedings of 36th Midwest Symposium on Circuits and Systems},
  pages={755--757},
  year={1993},
  organization={IEEE}
}

@article{chua1988cellularA,
  title={Cellular neural networks: Applications},
  author={Chua, Leon O and Yang, Lin},
  journal={IEEE Transactions on circuits and systems},
  volume={35},
  number={10},
  pages={1273--1290},
  year={1988},
  publisher={IEEE}
}

@article{chua1988cellular,
  title={Cellular neural networks: Theory},
  author={Chua, Leon O and Yang, Lin},
  journal={IEEE Transactions on circuits and systems},
  volume={35},
  number={10},
  pages={1257--1272},
  year={1988},
  publisher={IEEE}
}


@article{lee1990neural,
  title={Neural algorithm for solving differential equations},
  author={Lee, Hyuk and Kang, In Seok},
  journal={Journal of Computational Physics},
  volume={91},
  number={1},
  pages={110--131},
  year={1990},
  publisher={Elsevier}
}


@article{lagaris1998artificial,
  title={Artificial neural networks for solving ordinary and partial differential equations},
  author={Lagaris, Isaac E and Likas, Aristidis and Fotiadis, Dimitrios I},
  journal={IEEE transactions on neural networks},
  volume={9},
  number={5},
  pages={987--1000},
  year={1998},
  publisher={IEEE}
}

@article{simoncini2016computational,
  title={Computational methods for linear matrix equations},
  author={Simoncini, Valeria},
  journal={SIAM Review},
  volume={58},
  number={3},
  pages={377--441},
  year={2016},
  publisher={SIAM}
}

@article{guittet2015solving,
  title={Solving elliptic problems with discontinuities on irregular domains--the voronoi interface method},
  author={Guittet, Arthur and Lepilliez, Mathieu and Tanguy, Sebastien and Gibou, Fr{\'e}d{\'e}ric},
  journal={Journal of Computational Physics},
  volume={298},
  pages={747--765},
  year={2015},
  publisher={Elsevier}
}

@article{pakravan2021solving,
  title={Solving inverse-PDE problems with physics-aware neural networks},
  author={Pakravan, Samira and Mistani, Pouria A and Aragon-Calvo, Miguel A and Gibou, Frederic},
  journal={Journal of Computational Physics},
  volume={440},
  pages={110414},
  year={2021},
  publisher={Elsevier}
}

@article{sallee1984middle,
  title={The middle-cut triangulations of the n-cube},
  author={Sallee, John F},
  journal={SIAM Journal on Algebraic Discrete Methods},
  volume={5},
  number={3},
  pages={407--419},
  year={1984},
  publisher={SIAM}
}

@article{min2007geometric,
  title={Geometric integration over irregular domains with application to level-set methods},
  author={Min, Chohong and Gibou, Fr{\'e}d{\'e}ric},
  journal={Journal of Computational Physics},
  volume={226},
  number={2},
  pages={1432--1443},
  year={2007},
  publisher={Elsevier}
}


@article{MIN2007300,
title = {A second order accurate level set method on non-graded adaptive cartesian grids},
journal = {Journal of Computational Physics},
volume = {225},
number = {1},
pages = {300-321},
year = {2007},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2006.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S0021999106005912},
author = {Chohong Min and Frédéric Gibou},
keywords = {Level set method, Ghost fluid method, Adaptive mesh refinement, Non-graded Cartesian grids, Motion by mean curvature, Motion in the normal direction, Motion in an externally generated velocity field, Extrapolation in the normal direction},
abstract = {We present a level set method on non-graded adaptive Cartesian grids, i.e. grids for which the ratio between adjacent cells is not constrained. We use quadtree and octree data structures to represent the grid and a simple algorithm to generate a mesh with the finest resolution at the interface. In particular, we present (1) a locally third order accurate reinitialization scheme that transforms an arbitrary level set function into a signed distance function, (2) a second order accurate semi-Lagrangian methods to evolve the linear level set advection equation under an externally generated velocity field, (3) a second order accurate upwind method to evolve the non-linear level set equation under a normal velocity as well as to extrapolate scalar quantities across an interface in the normal direction, and (4) a semi-implicit scheme to evolve the interface under mean curvature. Combined, we obtain a level set method on adaptive Cartesian grids with a negligible amount of mass loss. We propose numerical examples in two and three spatial dimensions to demonstrate the accuracy of the method.}
}


@article{osher1988fronts,
  title={Fronts propagating with curvature-dependent speed: Algorithms based on Hamilton-Jacobi formulations},
  author={Osher, Stanley and Sethian, James A},
  journal={Journal of computational physics},
  volume={79},
  number={1},
  pages={12--49},
  year={1988},
  publisher={Elsevier}
}

@article{SUSSMAN1994146,
title = {A Level Set Approach for Computing Solutions to Incompressible Two-Phase Flow},
journal = {Journal of Computational Physics},
volume = {114},
number = {1},
pages = {146-159},
year = {1994},
issn = {0021-9991},
doi = {https://doi.org/10.1006/jcph.1994.1155},
url = {https://www.sciencedirect.com/science/article/pii/S0021999184711557},
author = {Mark Sussman and Peter Smereka and Stanley Osher},
abstract = {A level set approach for computing solutions to incompressible two-phase flow is presented. The interface between the two fluids is considered to be sharp and is described as the zero level set of a smooth function. We use a second-order projection method which implements a second-order upwinded procedure for differencing the convection terms. A new treatment of the level set method allows us to include large density and viscosity ratios as well as surface tension. We consider the motion of air bubbles in water and falling water drops in air.}
}

@article{XIU2001658,
title = {A Semi-Lagrangian High-Order Method for Navier–Stokes Equations},
journal = {Journal of Computational Physics},
volume = {172},
number = {2},
pages = {658-684},
year = {2001},
issn = {0021-9991},
doi = {https://doi.org/10.1006/jcph.2001.6847},
url = {https://www.sciencedirect.com/science/article/pii/S0021999101968470},
author = {Dongbin Xiu and George Em Karniadakis},
abstract = {We present a semi-Lagrangian method for advection–diffusion and incompressible Navier–Stokes equations. The focus is on constructing stable schemes of second-order temporal accuracy, as this is a crucial element for the successful application of semi-Lagrangian methods to turbulence simulations. We implement the method in the context of unstructured spectral/hp element discretization, which allows for efficient search-interpolation procedures as well as for illumination of the nonmonotonic behavior of the temporal (advection) error of the form: O(Δtk+Δxp+1Δt). We present numerical results that validate this error estimate for the advection–diffusion equation, and we document that such estimate is also valid for the Navier–Stokes equations at moderate or high Reynolds number. Two- and three-dimensional laminar and transitional flow simulations suggest that semi-Lagrangian schemes are more efficient than their Eulerian counterparts for high-order discretizations on nonuniform grids.}
}


@article{shu1988efficient,
  title={Efficient implementation of essentially non-oscillatory shock-capturing schemes},
  author={Shu, Chi-Wang and Osher, Stanley},
  journal={Journal of computational physics},
  volume={77},
  number={2},
  pages={439--471},
  year={1988},
  publisher={Elsevier}
}


@article{sylvester1884equation,
  title={Sur l’{\'e}quation en matrices px= xq},
  author={Sylvester, James Joseph},
  journal={CR Acad. Sci. Paris},
  volume={99},
  number={2},
  pages={67--71},
  year={1884}
}

@article{bochkov2020solving,
  title={Solving elliptic interface problems with jump conditions on Cartesian grids},
  author={Bochkov, Daniil and Gibou, Frederic},
  journal={Journal of Computational Physics},
  volume={407},
  pages={109269},
  year={2020},
  publisher={Elsevier}
}

@article{sgura2019parameter,
  title={Parameter estimation for a morphochemical reaction-diffusion model of electrochemical pattern formation},
  author={Sgura, Ivonne and Lawless, Amos S and Bozzini, Benedetto},
  journal={Inverse Problems in Science and Engineering},
  volume={27},
  number={5},
  pages={618--647},
  year={2019},
  publisher={Taylor \& Francis}
}

@article{ascher1995implicit,
  title={Implicit-explicit methods for time-dependent partial differential equations},
  author={Ascher, Uri M and Ruuth, Steven J and Wetton, Brian TR},
  journal={SIAM Journal on Numerical Analysis},
  volume={32},
  number={3},
  pages={797--823},
  year={1995},
  publisher={SIAM}
}

@article{hajarian2013matrix,
  title={Matrix iterative methods for solving the Sylvester-transpose and periodic Sylvester matrix equations},
  author={Hajarian, Masoud},
  journal={Journal of the Franklin Institute},
  volume={350},
  number={10},
  pages={3328--3341},
  year={2013},
  publisher={Elsevier}
}

@misc{gradClipping,
  doi = {10.48550/ARXIV.1211.5063},
  
  url = {https://arxiv.org/abs/1211.5063},
  
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {On the difficulty of training Recurrent Neural Networks},
  
  publisher = {arXiv},
  
  year = {2012},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@software{optax2020github,
  author = {Matteo Hessel and David Budden and Fabio Viola and Mihaela Rosca
            and Eren Sezener and Tom Hennigan},
  title = {Optax: composable gradient transformation and optimisation, in JAX!},
  url = {http://github.com/deepmind/optax},
  version = {0.0.1},
  year = {2020},
}


@inproceedings{hecht1987kolmogorov,
  title={Kolmogorov’s mapping neural network existence theorem},
  author={Hecht-Nielsen, Robert},
  booktitle={Proceedings of the international conference on Neural Networks},
  volume={3},
  pages={11--14},
  year={1987},
  organization={IEEE Press New York, NY, USA}
}


@article{sprecher1965structure,
  title={On the structure of continuous functions of several variables},
  author={Sprecher, David A},
  journal={Transactions of the American Mathematical Society},
  volume={115},
  pages={340--355},
  year={1965}
}

@inproceedings{kolmogorov1957representation,
  title={On the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition},
  author={Kolmogorov, Andrei Nikolaevich},
  booktitle={Doklady Akademii Nauk},
  volume={114},
  number={5},
  pages={953--956},
  year={1957},
  organization={Russian Academy of Sciences}
}


@misc{ismailov2022,
  doi = {10.48550/ARXIV.2012.03016},
  
  url = {https://arxiv.org/abs/2012.03016},
  
  author = {Ismailov, Vugar},
  
  keywords = {Machine Learning (cs.LG), Functional Analysis (math.FA), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics, 26B40, 46A22, 46E10, 46N60, 68T05, 92B20},
  
  title = {A three layer neural network can represent any multivariate function},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
