\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\Newlabel{cor}{1}
\citation{sharp1990calculating,MirzadehPB}
\citation{mistani2019parallel}
\citation{MISTANI2018150}
\citation{theillard2015sharp,bochkov2021sharp}
\citation{galatsis2010patterning,ouaknin2018level,bochkov2021non}
\Newlabel{1}{a}
\Newlabel{2}{b}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec::introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Problem statement}{1}{subsection.1.1}\protected@file@percent }
\citation{babuvska1970finite,bramble1996finite}
\citation{leveque1994immersed}
\citation{adams2002immersed,li2003new,ewing1999immersed,gong2008immersed}
\citation{fedkiw1999non}
\citation{liu2000boundary}
\citation{guittet2015solving}
\citation{crockett2011cartesian}
\citation{lew2008discontinuous,moes1999finite,belytschko2001arbitrary}
\citation{BOCHKOV2020109269}
\citation{lee1990neural}
\citation{gobovic1993design,chua1988cellular,chua1988cellularA}
\citation{lagaris1998artificial}
\citation{RAISSI2019686}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Literature on relevant finite discretization methods}{2}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Literature on solving PDEs with neural networks}{2}{subsection.1.3}\protected@file@percent }
\citation{SIRIGNANO20181339}
\citation{yu2018deep}
\citation{jax2018github}
\citation{holl2020phiflow}
\citation{Kochkov2021-ML-CFD}
\citation{bezgin2022jax}
\citation{pakravan2021solving,dal2020data,lu2020extracting}
\citation{berg2017neural}
\citation{um2020solver}
\citation{de2018end,holl2020learning}
\citation{chen1995universal}
\citation{lu2019deeponet,bhattacharya2020model,li2020neural,li2020fourier}
\citation{pakravan2021solving}
\@writefile{toc}{\contentsline {section}{\numberline {2}Neural Bootstrapping Method (NBM)}{3}{section.2}\protected@file@percent }
\newlabel{sec:nbm}{{2}{3}{Neural Bootstrapping Method (NBM)}{section.2}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:nbm_kernel}{{1a}{4}{NBM kernels compute residual contribution by each collocation point per thread. Kernel operations involve considering implicit cells at different resolutions according to the bootstrapped finite discretization method.\relax }{figure.caption.1}{}}
\newlabel{sub@fig:nbm_kernel}{{a}{4}{NBM kernels compute residual contribution by each collocation point per thread. Kernel operations involve considering implicit cells at different resolutions according to the bootstrapped finite discretization method.\relax }{figure.caption.1}{}}
\newlabel{fig:trainingloop}{{1b}{4}{NBM outer training layout. Geometric information is managed by a mesh oracle that is often a structured mesh at much lower resolution that stores the level-set funcion. The pointwise evaluations at each implicit cell is locally preconditioned based on the geometry of the interfaces crossing the implicit cells. The training loop involves automatic differentiation across the assembly of the linear system performed by the NBM kernels.\relax }{figure.caption.1}{}}
\newlabel{sub@fig:trainingloop}{{b}{4}{NBM outer training layout. Geometric information is managed by a mesh oracle that is often a structured mesh at much lower resolution that stores the level-set funcion. The pointwise evaluations at each implicit cell is locally preconditioned based on the geometry of the interfaces crossing the implicit cells. The training loop involves automatic differentiation across the assembly of the linear system performed by the NBM kernels.\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Neural Bootstrapping Method (NBM).\relax }}{4}{figure.caption.1}\protected@file@percent }
\newlabel{fig:nbm}{{1}{4}{Neural Bootstrapping Method (NBM).\relax }{figure.caption.1}{}}
\citation{shallue2018measuring}
\citation{MIN2007300}
\@writefile{toc}{\contentsline {section}{\numberline {3}JAX-DIPS: Differentiable Interfacial PDE Solver}{5}{section.3}\protected@file@percent }
\newlabel{sec:dips}{{3}{5}{JAX-DIPS: Differentiable Interfacial PDE Solver}{section.3}{}}
\citation{MIN2007300}
\citation{osher1988fronts}
\citation{min2007geometric}
\citation{sallee1984middle}
\citation{min2007geometric}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Interpolation methods}{6}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Trilinear interpolation}{6}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Quadratic non-oscillatory interpolation}{6}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Level-set method}{6}{subsection.3.2}\protected@file@percent }
\citation{min2007geometric}
\citation{min2007geometric}
\citation{min2007geometric}
\citation{min2007geometric}
\citation{min2007geometric}
\citation{min2007geometric}
\citation{min2007geometric}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Geometric integration}{7}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Integration over 3D surfaces and volumes}{7}{subsubsection.3.3.1}\protected@file@percent }
\citation{hecht1987kolmogorov}
\citation{kolmogorov1957representation}
\citation{sprecher1965structure}
\citation{ismailov2022}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Cross sections of interface with grid cell faces}{8}{subsubsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Neural network approximators for the solution}{8}{subsection.3.4}\protected@file@percent }
\citation{kingma2014adam}
\citation{bochkov2020solving}
\citation{bochkov2020solving}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Two neural networks are defined for the two regions of the computational domain.\relax }}{9}{figure.caption.2}\protected@file@percent }
\newlabel{fig:shapes}{{2}{9}{Two neural networks are defined for the two regions of the computational domain.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Approach I. Finite discretization method fused with regression-based extrapolation}{9}{subsection.3.5}\protected@file@percent }
\newlabel{sec::FD}{{3.5}{9}{Approach I. Finite discretization method fused with regression-based extrapolation}{subsection.3.5}{}}
\citation{bochkov2020solving}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Notation used in this paper. Close to the interface where finite volumes are crossed by the interface, there are extra degrees of freedom (open circles) that are extrapolations of solutions from each domain to the opposite domain. Jump conditions are implicitly encoded in these extrapolated values.\relax }}{10}{figure.caption.3}\protected@file@percent }
\newlabel{fig:grid}{{3}{10}{Notation used in this paper. Close to the interface where finite volumes are crossed by the interface, there are extra degrees of freedom (open circles) that are extrapolations of solutions from each domain to the opposite domain. Jump conditions are implicitly encoded in these extrapolated values.\relax }{figure.caption.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Bias Slow approximation of the non-existing solution value on a grid point based on existing solution values in its neighborhood. The notation is used for $u_{i,j}^\pm =B_{i,j}^\pm : \mathbf  {U}_{i,j}+r_{i,j}^\pm $.\relax }}{13}{algorithm.1}\protected@file@percent }
\newlabel{euclid}{{1}{13}{Bias Slow approximation of the non-existing solution value on a grid point based on existing solution values in its neighborhood. The notation is used for $u_{i,j}^\pm =B_{i,j}^\pm : \mathbf {U}_{i,j}+r_{i,j}^\pm $.\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Approach II. Finite discretization method fused with neural extrapolation}{14}{subsection.3.6}\protected@file@percent }
\newlabel{sec:fusion}{{3.6}{14}{Approach II. Finite discretization method fused with neural extrapolation}{subsection.3.6}{}}
\newlabel{eq::taylorexpandjump}{{10}{14}{Approach II. Finite discretization method fused with neural extrapolation}{equation.3.10}{}}
\newlabel{eq::extrapolate1}{{11}{14}{Approach II. Finite discretization method fused with neural extrapolation}{equation.3.11}{}}
\newlabel{eq::extrapolate2}{{12}{14}{Approach II. Finite discretization method fused with neural extrapolation}{equation.3.12}{}}
\citation{kingma2014adam}
\citation{optax2020github}
\citation{gradClipping}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Optimization scheme}{15}{subsection.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.7.1}Preconditioners are ideal network regularizers}{15}{subsubsection.3.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.7.2}Learning rate scheduling}{15}{subsubsection.3.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.7.3}Domain switching optimization scheme}{16}{subsubsection.3.7.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Domain switching method. Switching interval is $\tau $.\relax }}{16}{algorithm.2}\protected@file@percent }
\newlabel{switching}{{2}{16}{Domain switching method. Switching interval is $\tau $.\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.7.4}Multi-GPU parallelization with model parallel training}{16}{subsubsection.3.7.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Effect of shuffling on the star example of \ref  {subsec:star}; (left) without shuffling and (right) with shuffling. Shuffling has to be applied on the batches when training on batched data points.\relax }}{17}{figure.caption.4}\protected@file@percent }
\newlabel{fig:batcheffect}{{4}{17}{Effect of shuffling on the star example of \ref {subsec:star}; (left) without shuffling and (right) with shuffling. Shuffling has to be applied on the batches when training on batched data points.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Numerical Results}{17}{section.4}\protected@file@percent }
\newlabel{sec:examples}{{4}{17}{Numerical Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Accuracy on spherical interface: single-resolution, single batch, single GPU}{17}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Loss evolution with epochs for the sphere of $16\times 16\times 16$ grid (left) and different accuracy measures, RMSE and $L^\infty $, at 5 different resolutions (right).\relax }}{18}{figure.caption.5}\protected@file@percent }
\newlabel{fig:losses}{{5}{18}{Loss evolution with epochs for the sphere of $16\times 16\times 16$ grid (left) and different accuracy measures, RMSE and $L^\infty $, at 5 different resolutions (right).\relax }{figure.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Convergence and timings for the sphere example averaged over $10,000$ epochs. Timings include the initial compilation time. Measurements are on a single NVIDIA A6000 GPU. The regression-based method has 5 hidden layers with 10 neurons each, overall 928 trainable parameters.\relax }}{18}{table.caption.6}\protected@file@percent }
\newlabel{tab:convergence_sphere}{{1}{18}{Convergence and timings for the sphere example averaged over $10,000$ epochs. Timings include the initial compilation time. Measurements are on a single NVIDIA A6000 GPU. The regression-based method has 5 hidden layers with 10 neurons each, overall 928 trainable parameters.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Accuracy on star interface: single GPU, domain switching, neural extrapolation, and batching}{18}{subsection.4.2}\protected@file@percent }
\newlabel{subsec:star}{{4.2}{18}{Accuracy on star interface: single GPU, domain switching, neural extrapolation, and batching}{subsection.4.2}{}}
\newlabel{subfig:sphere}{{6a}{19}{Illustration of numerical solution and absolute error on a cross section of the domain. \relax }{figure.caption.7}{}}
\newlabel{sub@subfig:sphere}{{a}{19}{Illustration of numerical solution and absolute error on a cross section of the domain. \relax }{figure.caption.7}{}}
\newlabel{fig:spheregrad}{{6b}{19}{Streamlines of solution gradient for (left) the surrogate neural model colored by model solution value, (right) exact streamlines colored by exact solution values. \relax }{figure.caption.7}{}}
\newlabel{sub@fig:spheregrad}{{b}{19}{Streamlines of solution gradient for (left) the surrogate neural model colored by model solution value, (right) exact streamlines colored by exact solution values. \relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The neural network surrogate model trained on a $128^3$ grid using a single NVIDIA A6000 GPU.\relax }}{19}{figure.caption.7}\protected@file@percent }
\newlabel{fig:sphere}{{6}{19}{The neural network surrogate model trained on a $128^3$ grid using a single NVIDIA A6000 GPU.\relax }{figure.caption.7}{}}
\citation{curless1996volumetric}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Convergence in solution of the star geometry using the single-resolution regression-based solver with domain switching. We report $L^\infty $-norm error as well as root-mean-squared-error (RMSE) of the solution field evaluated everywhere in the domain. Timings are averaged over $10,000$ epochs in each case and include the initial compilation time for jaxpressions. The neural network pair have 1 hidden layer each with 100 neurons, overall $1,002$ trainable parameteres. Domain switching scheme follows the $\textrm  {whole region} \rightarrow \textrm  {fast region} \rightarrow \textrm  {fast region}$ sequence.\relax }}{20}{table.caption.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Convergence in solution of the star geometry using the multi-resolution regression-based solver with batching. We use a multi-resolution training protocol that refines to 4 levels at each collocation point. Batch size is the minimum of $64\times 64\times 32$ and number of collocation points, which ensures memory saturation at $\nobreakspace  {}30$ GB.\relax }}{20}{table.caption.8}\protected@file@percent }
\newlabel{subfig:star}{{7a}{21}{Illustration of three dimensional interface used (left), and $\mu ^\pm $ on the $32\times 32\times 32$ grid (right).\relax }{figure.caption.9}{}}
\newlabel{sub@subfig:star}{{a}{21}{Illustration of three dimensional interface used (left), and $\mu ^\pm $ on the $32\times 32\times 32$ grid (right).\relax }{figure.caption.9}{}}
\newlabel{fig:lossestar}{{7b}{21}{Loss evolution with epochs for the star of $64\times 64\times 64$ grid using domain switching training (left), and decrease in error by increasing resolutions (right).\relax }{figure.caption.9}{}}
\newlabel{sub@fig:lossestar}{{b}{21}{Loss evolution with epochs for the star of $64\times 64\times 64$ grid using domain switching training (left), and decrease in error by increasing resolutions (right).\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The neural network model trained with different configurations and resolutions.\relax }}{21}{figure.caption.9}\protected@file@percent }
\newlabel{fig:star}{{7}{21}{The neural network model trained with different configurations and resolutions.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Illustration of exact and numerical solutions (top row) and gradient streamlines (bottom row) on a $64\times 64 \times 64$ grid.\relax }}{22}{figure.caption.10}\protected@file@percent }
\newlabel{fig:star_sol}{{8}{22}{Illustration of exact and numerical solutions (top row) and gradient streamlines (bottom row) on a $64\times 64 \times 64$ grid.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Time complexity and parallel scaling on GPU clusters}{23}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Scaling test. Time per epoch (sec) and JAX compile time for different configurations.\relax }}{23}{table.caption.11}\protected@file@percent }
\newlabel{tab:scaling}{{4}{23}{Scaling test. Time per epoch (sec) and JAX compile time for different configurations.\relax }{table.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{23}{section.5}\protected@file@percent }
\newlabel{sec:conc}{{5}{23}{Conclusion}{section.5}{}}
\newlabel{fig:dragon1}{{9a}{24}{Geometry of the dragon and gradient streamlines, colored by solution values.\relax }{figure.caption.12}{}}
\newlabel{sub@fig:dragon1}{{a}{24}{Geometry of the dragon and gradient streamlines, colored by solution values.\relax }{figure.caption.12}{}}
\newlabel{fig:dragon2}{{9b}{24}{Jump in solution and its gradient.\relax }{figure.caption.12}{}}
\newlabel{sub@fig:dragon2}{{b}{24}{Jump in solution and its gradient.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The NBM approach enables a $1024^3$ effective resolution on a single NVIDIA A6000 GPU.\relax }}{24}{figure.caption.12}\protected@file@percent }
\newlabel{fig:dragon}{{9}{24}{The NBM approach enables a $1024^3$ effective resolution on a single NVIDIA A6000 GPU.\relax }{figure.caption.12}{}}
\bibstyle{abbrv}
\bibdata{references}
\bibcite{adams2002immersed}{{1}{}{{}}{{}}}
\bibcite{ANDRIENKO2018520}{{2}{}{{}}{{}}}
\bibcite{babuvska1970finite}{{3}{}{{}}{{}}}
\bibcite{belytschko2001arbitrary}{{4}{}{{}}{{}}}
\bibcite{berg2017neural}{{5}{}{{}}{{}}}
\bibcite{bezgin2022jax}{{6}{}{{}}{{}}}
\bibcite{bhattacharya2020model}{{7}{}{{}}{{}}}
\bibcite{BOCHKOV2020109269}{{8}{}{{}}{{}}}
\bibcite{bochkov2020solving}{{9}{}{{}}{{}}}
\bibcite{bochkov2021non}{{10}{}{{}}{{}}}
\bibcite{bochkov2021sharp}{{11}{}{{}}{{}}}
\bibcite{jax2018github}{{12}{}{{}}{{}}}
\bibcite{bramble1996finite}{{13}{}{{}}{{}}}
\bibcite{chen1995universal}{{14}{}{{}}{{}}}
\bibcite{chua1988cellularA}{{15}{}{{}}{{}}}
\bibcite{chua1988cellular}{{16}{}{{}}{{}}}
\bibcite{crockett2011cartesian}{{17}{}{{}}{{}}}
\bibcite{curless1996volumetric}{{18}{}{{}}{{}}}
\bibcite{dal2020data}{{19}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{References}{25}{section*.13}\protected@file@percent }
\bibcite{de2018end}{{20}{}{{}}{{}}}
\bibcite{ewing1999immersed}{{21}{}{{}}{{}}}
\bibcite{fedkiw1999non}{{22}{}{{}}{{}}}
\bibcite{galatsis2010patterning}{{23}{}{{}}{{}}}
\bibcite{gobovic1993design}{{24}{}{{}}{{}}}
\bibcite{gong2008immersed}{{25}{}{{}}{{}}}
\bibcite{guittet2015solving}{{26}{}{{}}{{}}}
\bibcite{hecht1987kolmogorov}{{27}{}{{}}{{}}}
\bibcite{optax2020github}{{28}{}{{}}{{}}}
\bibcite{holl2020learning}{{29}{}{{}}{{}}}
\bibcite{holl2020phiflow}{{30}{}{{}}{{}}}
\bibcite{ismailov2022}{{31}{}{{}}{{}}}
\bibcite{kingma2014adam}{{32}{}{{}}{{}}}
\bibcite{Kochkov2021-ML-CFD}{{33}{}{{}}{{}}}
\bibcite{kolmogorov1957representation}{{34}{}{{}}{{}}}
\bibcite{lagaris1998artificial}{{35}{}{{}}{{}}}
\bibcite{lee1990neural}{{36}{}{{}}{{}}}
\bibcite{leveque1994immersed}{{37}{}{{}}{{}}}
\bibcite{lew2008discontinuous}{{38}{}{{}}{{}}}
\bibcite{li2020fourier}{{39}{}{{}}{{}}}
\bibcite{li2020neural}{{40}{}{{}}{{}}}
\bibcite{li2003new}{{41}{}{{}}{{}}}
\bibcite{liu2000boundary}{{42}{}{{}}{{}}}
\bibcite{lu2019deeponet}{{43}{}{{}}{{}}}
\bibcite{lu2020extracting}{{44}{}{{}}{{}}}
\bibcite{min2007geometric}{{45}{}{{}}{{}}}
\bibcite{MIN2007300}{{46}{}{{}}{{}}}
\bibcite{MirzadehPB}{{47}{}{{}}{{}}}
\bibcite{MISTANI2018150}{{48}{}{{}}{{}}}
\bibcite{mistani2019parallel}{{49}{}{{}}{{}}}
\bibcite{moes1999finite}{{50}{}{{}}{{}}}
\bibcite{osher1988fronts}{{51}{}{{}}{{}}}
\bibcite{oswald2005smectic}{{52}{}{{}}{{}}}
\bibcite{ouaknin2018level}{{53}{}{{}}{{}}}
\bibcite{pakravan2021solving}{{54}{}{{}}{{}}}
\bibcite{gradClipping}{{55}{}{{}}{{}}}
\bibcite{RAISSI2019686}{{56}{}{{}}{{}}}
\bibcite{sallee1984middle}{{57}{}{{}}{{}}}
\bibcite{shallue2018measuring}{{58}{}{{}}{{}}}
\bibcite{sharp1990calculating}{{59}{}{{}}{{}}}
\bibcite{shu1988efficient}{{60}{}{{}}{{}}}
\bibcite{SIRIGNANO20181339}{{61}{}{{}}{{}}}
\bibcite{sprecher1965structure}{{62}{}{{}}{{}}}
\bibcite{SUSSMAN1994146}{{63}{}{{}}{{}}}
\bibcite{theillard2015sharp}{{64}{}{{}}{{}}}
\bibcite{um2020solver}{{65}{}{{}}{{}}}
\bibcite{XIU2001658}{{66}{}{{}}{{}}}
\bibcite{yu2018deep}{{67}{}{{}}{{}}}
\@writefile{toc}{\let\numberline\tmptocnumberline}
\@writefile{toc}{\contentsline {section}{\numberline {Appendix \nobreakspace  {}A}Solving interface problems with physics-informed neural networks}{28}{appendix.A}\protected@file@percent }
\newlabel{eq::interfaceLoss}{{A.2}{28}{Solving interface problems with physics-informed neural networks}{equation.A.2}{}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
