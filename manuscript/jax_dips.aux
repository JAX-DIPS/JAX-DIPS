\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\Newlabel{cor}{1}
\Newlabel{1}{a}
\Newlabel{2}{b}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec::introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{MIN2007300}
\citation{MIN2007300}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Life sciences are described by elliptic PDEs with free interfaces}{2}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Level-set method for free boundary problems}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Interpolation methods}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Trilinear interpolation}{2}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Quadratic non-oscillatory interpolation}{2}{subsubsection.2.1.2}\protected@file@percent }
\citation{osher1988fronts}
\citation{SUSSMAN1994146}
\citation{XIU2001658}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Level-set method}{3}{subsection.2.2}\protected@file@percent }
\newlabel{eq::sussman}{{2}{3}{Level-set method}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Second order accurate semi-Lagrangian advection scheme}{3}{subsubsection.2.2.1}\protected@file@percent }
\citation{shu1988efficient}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Godunov Hamiltonian for reinitialization}{4}{subsubsection.2.2.2}\protected@file@percent }
\citation{shu1988efficient}
\citation{min2007geometric}
\citation{sallee1984middle}
\citation{min2007geometric}
\citation{min2007geometric}
\citation{min2007geometric}
\citation{min2007geometric}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Geometric operations}{5}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Integration over 3D surfaces and volumes}{5}{subsubsection.2.3.1}\protected@file@percent }
\citation{min2007geometric}
\citation{min2007geometric}
\citation{min2007geometric}
\citation{min2007geometric}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Cross sections of interface with grid cell faces}{6}{subsubsection.2.3.2}\protected@file@percent }
\citation{hecht1987kolmogorov}
\citation{kolmogorov1957representation}
\citation{sprecher1965structure}
\citation{ismailov2022}
\@writefile{toc}{\contentsline {section}{\numberline {3}Solving Interfacial PDEs with Differentiable Residual Minimization (DRM)}{7}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Fully classic numerical solver is used to train neural surrogate models. Training occurs in the finite dimensional space spanned by the finite discretization methods.\relax }}{7}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:proposal}{{1}{7}{Fully classic numerical solver is used to train neural surrogate models. Training occurs in the finite dimensional space spanned by the finite discretization methods.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Neural network approximators for the solution}{7}{subsection.3.1}\protected@file@percent }
\citation{kingma2014adam}
\citation{bochkov2020solving}
\citation{bochkov2020solving}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Two neural networks are defined for the two regions of the computational domain.\relax }}{8}{figure.caption.2}\protected@file@percent }
\newlabel{fig:shapes}{{2}{8}{Two neural networks are defined for the two regions of the computational domain.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Approach I. Finite discretization method fused with regression extrapolation}{8}{subsection.3.2}\protected@file@percent }
\newlabel{sec::FD}{{3.2}{8}{Approach I. Finite discretization method fused with regression extrapolation}{subsection.3.2}{}}
\citation{bochkov2020solving}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Notation used in this paper. Close to the interface where finite volumes are crossed by the interface, there are extra degrees of freedom (open circles) that are extrapolations of solutions from each domain to the opposite domain. Jump conditions are implicitly encoded in these extrapolated values.\relax }}{9}{figure.caption.3}\protected@file@percent }
\newlabel{fig:grid}{{3}{9}{Notation used in this paper. Close to the interface where finite volumes are crossed by the interface, there are extra degrees of freedom (open circles) that are extrapolations of solutions from each domain to the opposite domain. Jump conditions are implicitly encoded in these extrapolated values.\relax }{figure.caption.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Bias Slow approximation of the non-existing solution value on a grid point based on existing solution values in its neighborhood. The notation is used for $u_{i,j}^\pm =B_{i,j}^\pm : \mathbf  {U}_{i,j}+r_{i,j}^\pm $.\relax }}{12}{algorithm.1}\protected@file@percent }
\newlabel{euclid}{{1}{12}{Bias Slow approximation of the non-existing solution value on a grid point based on existing solution values in its neighborhood. The notation is used for $u_{i,j}^\pm =B_{i,j}^\pm : \mathbf {U}_{i,j}+r_{i,j}^\pm $.\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Approach II. Finite discretization method fused with neural extrapolation}{13}{subsection.3.3}\protected@file@percent }
\newlabel{eq::taylorexpandjump}{{13}{13}{Approach II. Finite discretization method fused with neural extrapolation}{equation.3.13}{}}
\newlabel{eq::extrapolate1}{{14}{13}{Approach II. Finite discretization method fused with neural extrapolation}{equation.3.14}{}}
\newlabel{eq::extrapolate2}{{15}{13}{Approach II. Finite discretization method fused with neural extrapolation}{equation.3.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Approach III. Penalty minimization method}{14}{subsection.3.4}\protected@file@percent }
\newlabel{eq::interfaceLoss}{{17}{14}{Approach III. Penalty minimization method}{equation.3.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Optimization scheme}{14}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Preconditioners are ideal network regularizers}{14}{subsection.4.1}\protected@file@percent }
\citation{optax2020github}
\citation{kingma2014adam}
\citation{gradClipping}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Learning rate scheduling}{15}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Domain switching optimization scheme}{15}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Examples}{15}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Case I.}{15}{subsection.5.1}\protected@file@percent }
\citation{guittet2015solving}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Domain switching method. Switching interval is $\tau $.\relax }}{16}{algorithm.2}\protected@file@percent }
\newlabel{switching}{{2}{16}{Domain switching method. Switching interval is $\tau $.\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Case II.}{16}{subsection.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Loss evolution with epochs for the sphere of $16\times 16\times 16$ grid (left) and different accuracy measures at 5 resolutions (right).\relax }}{17}{figure.caption.5}\protected@file@percent }
\newlabel{fig:losses}{{4}{17}{Loss evolution with epochs for the sphere of $16\times 16\times 16$ grid (left) and different accuracy measures at 5 resolutions (right).\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Illustration of three dimensional interface used (left), and $\mu ^\pm $ on the $16\times 16\times 16$ grid (right).\relax }}{17}{figure.caption.6}\protected@file@percent }
\newlabel{fig:sphere}{{5}{17}{Illustration of three dimensional interface used (left), and $\mu ^\pm $ on the $16\times 16\times 16$ grid (right).\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Illustration of numerical solution and absolute error using the regression based solver on a cross section of the domain. \relax }}{18}{figure.caption.7}\protected@file@percent }
\newlabel{fig:sphere}{{6}{18}{Illustration of numerical solution and absolute error using the regression based solver on a cross section of the domain. \relax }{figure.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Convergence on the solution using the regression-based solver. We report $L^\infty $-norm error as well as root-mean-squared-error (RMSE) of the solution field evaluated everywhere in the domain. Rightmost column reports the overall time to solution for \texttt  {JAX-DIPS} which constitutes $10,000$ epochs in each case and the initial compilation time of jaxpressions. The neural network has $982$ trainable parameters. In each case GPU compute occupancy is at $100\%$ on a single NVIDIA RTX A6000 GPU. The neural method has 2 hidden layers with 10 neurons each, overall 322 trainable parameteres. The regression-based method has 5 hidden layers with 10 neurons each, overall 928 trainable parameters.\relax }}{19}{table.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion \& Future Directions}{19}{section.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Streamlines of solution gradient for (left) the surrogate neural model colored by model solution value, (right) exact streamlines colored by exact solution values. \relax }}{20}{figure.caption.8}\protected@file@percent }
\newlabel{fig:spheregrad}{{7}{20}{Streamlines of solution gradient for (left) the surrogate neural model colored by model solution value, (right) exact streamlines colored by exact solution values. \relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Illustration of three dimensional interface used (left), and $\mu ^\pm $ on the $32\times 32\times 32$ grid (right).\relax }}{20}{figure.caption.10}\protected@file@percent }
\newlabel{fig:star}{{8}{20}{Illustration of three dimensional interface used (left), and $\mu ^\pm $ on the $32\times 32\times 32$ grid (right).\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Illustration of exact and numerical solutions on a $64\times 64 \times 64$ grid.\relax }}{21}{figure.caption.11}\protected@file@percent }
\newlabel{fig:star_sol}{{9}{21}{Illustration of exact and numerical solutions on a $64\times 64 \times 64$ grid.\relax }{figure.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Convergence on the solution using the regression-based solver. We report $L^\infty $-norm error as well as root-mean-squared-error (RMSE) of the solution field evaluated everywhere in the domain. Rightmost column reports the overall time to solution for \texttt  {JAX-DIPS} which constitutes $10,000$ epochs in each case and the initial compilation time of jaxpressions. The neural network has $982$ trainable parameters. In each case GPU compute occupancy is at $100\%$ on a single NVIDIA RTX A6000 GPU. The neural method has 2 hidden layers with 10 neurons each, overall 322 trainable parameteres. The regression-based method has 5 hidden layers with 10 neurons each, overall 928 trainable parameters. We use a domain switching scheme with the $\textrm  {whole region} \rightarrow \textrm  {fast region} \rightarrow \textrm  {fast region}$ optimization sequence.\relax }}{22}{table.caption.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Loss evolution with epochs for the sphere of $64\times 64\times 64$ grid (left) and different accuracy measures at 5 resolutions (right).\relax }}{22}{figure.caption.12}\protected@file@percent }
\newlabel{fig:lossestar}{{10}{22}{Loss evolution with epochs for the sphere of $64\times 64\times 64$ grid (left) and different accuracy measures at 5 resolutions (right).\relax }{figure.caption.12}{}}
\bibstyle{abbrv}
\bibdata{references}
\bibcite{bochkov2020solving}{{1}{}{{}}{{}}}
\bibcite{guittet2015solving}{{2}{}{{}}{{}}}
\bibcite{hecht1987kolmogorov}{{3}{}{{}}{{}}}
\bibcite{optax2020github}{{4}{}{{}}{{}}}
\bibcite{ismailov2022}{{5}{}{{}}{{}}}
\bibcite{kingma2014adam}{{6}{}{{}}{{}}}
\bibcite{kolmogorov1957representation}{{7}{}{{}}{{}}}
\bibcite{min2007geometric}{{8}{}{{}}{{}}}
\bibcite{MIN2007300}{{9}{}{{}}{{}}}
\bibcite{osher1988fronts}{{10}{}{{}}{{}}}
\bibcite{pakravan2021solving}{{11}{}{{}}{{}}}
\bibcite{gradClipping}{{12}{}{{}}{{}}}
\bibcite{sallee1984middle}{{13}{}{{}}{{}}}
\bibcite{shu1988efficient}{{14}{}{{}}{{}}}
\bibcite{sprecher1965structure}{{15}{}{{}}{{}}}
\bibcite{SUSSMAN1994146}{{16}{}{{}}{{}}}
\bibcite{XIU2001658}{{17}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\contentsline {section}{References}{24}{section*.13}\protected@file@percent }
