\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{tiny-cuda-nn}
\citation{tiny-cuda-nn}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The proposed Neural Bootstrapping Method applied to electrostatics. Left: streamlines. Right: warped cross-section showing the physically correct jump in the field. The neural approach readily enables a $1024^3$ effective resolution on a single NVIDIA A6000 GPU. Once trained, it takes sub-milliseconds for the network to evaluate such a simulation \cite  {tiny-cuda-nn}.\relax }}{1}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:dragon}{{1}{1}{The proposed Neural Bootstrapping Method applied to electrostatics. Left: streamlines. Right: warped cross-section showing the physically correct jump in the field. The neural approach readily enables a $1024^3$ effective resolution on a single NVIDIA A6000 GPU. Once trained, it takes sub-milliseconds for the network to evaluate such a simulation \cite {tiny-cuda-nn}.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{lee1990neural}
\citation{gobovic1993design,chua1988cellular,chua1988cellularA}
\citation{lagaris1998artificial}
\citation{RAISSI2019686}
\citation{wang2022and,rahaman2019spectral}
\citation{SIRIGNANO20181339}
\citation{yu2018deep}
\citation{jax2018github}
\citation{holl2020phiflow}
\citation{Kochkov2021-ML-CFD}
\citation{bezgin2022jax}
\citation{pakravan2021solving,dal2020data,lu2020extracting}
\citation{berg2017neural}
\citation{um2020solver}
\citation{de2018end,holl2020learning}
\citation{chen1995universal}
\citation{lu2019deeponet,bhattacharya2020model,li2020neural,li2020fourier}
\citation{pakravan2021solving}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem statement}{2}{section.2}\protected@file@percent }
\citation{sharp1990calculating,MirzadehPB}
\citation{mistani2019parallel}
\citation{MISTANI2018150}
\citation{theillard2015sharp,bochkov2021sharp}
\citation{galatsis2010patterning,ouaknin2018level,bochkov2021non}
\citation{shallue2018measuring}
\citation{BOCHKOV2020109269}
\citation{osher1988fronts}
\citation{kingma2014adam}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Neural Bootstrapping Method (NBM). NBM kernels compute the residual contribution at each collocation point per GPU thread. Kernel operations involve placing implicit cells at multi-resolutions and calculating interface-cell crossings. Point-wise residuals are locally preconditioned ($M^{-1}$ matrix). $A,b$ are the left/right-hand sides of the linear system obtained by finite discretization method. $u$ is the vector of solution values predicted by the neural network model at the stencil points.\relax }}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig:nbm}{{2}{3}{Neural Bootstrapping Method (NBM). NBM kernels compute the residual contribution at each collocation point per GPU thread. Kernel operations involve placing implicit cells at multi-resolutions and calculating interface-cell crossings. Point-wise residuals are locally preconditioned ($M^{-1}$ matrix). $A,b$ are the left/right-hand sides of the linear system obtained by finite discretization method. $u$ is the vector of solution values predicted by the neural network model at the stencil points.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Scalable and Mesh-Freeing Neuro-Symbolic Differential Solver}{3}{section.3}\protected@file@percent }
\citation{curless1996volumetric}
\@writefile{toc}{\contentsline {section}{\numberline {4}Numerical results}{4}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Convergence and accuracy}{4}{subsection.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Convergence and overall time to solution for our JAX implementation, with $10,000$ epochs in each case, and initial compilation time. Measurements are on a single NVIDIA A6000 GPU. \relax }}{4}{table.caption.4}\protected@file@percent }
\newlabel{tab:convergence}{{1}{4}{Convergence and overall time to solution for our JAX implementation, with $10,000$ epochs in each case, and initial compilation time. Measurements are on a single NVIDIA A6000 GPU. \relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Time complexity and parallel scaling on GPU clusters}{4}{subsection.4.2}\protected@file@percent }
\newlabel{sec:scaling}{{4.2}{4}{Time complexity and parallel scaling on GPU clusters}{subsection.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Scaling test. Time per epoch (sec) and JAX compile time for different configurations.\relax }}{4}{table.caption.5}\protected@file@percent }
\newlabel{tab:scaling}{{2}{4}{Scaling test. Time per epoch (sec) and JAX compile time for different configurations.\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{4}{section.5}\protected@file@percent }
\bibstyle{abbrv}
\bibdata{references}
\bibcite{berg2017neural}{{1}{}{{}}{{}}}
\bibcite{bezgin2022jax}{{2}{}{{}}{{}}}
\bibcite{bhattacharya2020model}{{3}{}{{}}{{}}}
\bibcite{BOCHKOV2020109269}{{4}{}{{}}{{}}}
\bibcite{bochkov2021non}{{5}{}{{}}{{}}}
\bibcite{bochkov2021sharp}{{6}{}{{}}{{}}}
\bibcite{jax2018github}{{7}{}{{}}{{}}}
\bibcite{chen1995universal}{{8}{}{{}}{{}}}
\bibcite{chua1988cellularA}{{9}{}{{}}{{}}}
\bibcite{chua1988cellular}{{10}{}{{}}{{}}}
\bibcite{curless1996volumetric}{{11}{}{{}}{{}}}
\bibcite{dal2020data}{{12}{}{{}}{{}}}
\bibcite{de2018end}{{13}{}{{}}{{}}}
\bibcite{galatsis2010patterning}{{14}{}{{}}{{}}}
\bibcite{gobovic1993design}{{15}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{References}{5}{section*.7}\protected@file@percent }
\bibcite{holl2020learning}{{16}{}{{}}{{}}}
\bibcite{holl2020phiflow}{{17}{}{{}}{{}}}
\bibcite{kingma2014adam}{{18}{}{{}}{{}}}
\bibcite{Kochkov2021-ML-CFD}{{19}{}{{}}{{}}}
\bibcite{lagaris1998artificial}{{20}{}{{}}{{}}}
\bibcite{lee1990neural}{{21}{}{{}}{{}}}
\bibcite{li2020fourier}{{22}{}{{}}{{}}}
\bibcite{li2020neural}{{23}{}{{}}{{}}}
\bibcite{lu2019deeponet}{{24}{}{{}}{{}}}
\bibcite{lu2020extracting}{{25}{}{{}}{{}}}
\bibcite{MirzadehPB}{{26}{}{{}}{{}}}
\bibcite{MISTANI2018150}{{27}{}{{}}{{}}}
\bibcite{mistani2019parallel}{{28}{}{{}}{{}}}
\bibcite{tiny-cuda-nn}{{29}{}{{}}{{}}}
\bibcite{osher1988fronts}{{30}{}{{}}{{}}}
\bibcite{ouaknin2018level}{{31}{}{{}}{{}}}
\bibcite{pakravan2021solving}{{32}{}{{}}{{}}}
\bibcite{rahaman2019spectral}{{33}{}{{}}{{}}}
\bibcite{RAISSI2019686}{{34}{}{{}}{{}}}
\bibcite{shallue2018measuring}{{35}{}{{}}{{}}}
\bibcite{sharp1990calculating}{{36}{}{{}}{{}}}
\bibcite{SIRIGNANO20181339}{{37}{}{{}}{{}}}
\bibcite{theillard2015sharp}{{38}{}{{}}{{}}}
\bibcite{um2020solver}{{39}{}{{}}{{}}}
\bibcite{wang2022and}{{40}{}{{}}{{}}}
\bibcite{yu2018deep}{{41}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
