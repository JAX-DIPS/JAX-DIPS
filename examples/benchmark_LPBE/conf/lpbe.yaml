expname: benchmark-lpbe-app

defaults:
  - base_hydra_config
  - solver: poisson
  - _self_

# customization follows
gridstates:
  Nx: 256  # grid for level-set
  Ny: 256  # grid for level-set
  Nz: 256  # grid for level-set
  Nx_eval: 256  # grid for evaluation/visualization
  Ny_eval: 256  # grid for evaluation/visualization
  Nz_eval: 256  # grid for evaluation/visualization

solver:
  Nx_tr: 32  # grid for training
  Ny_tr: 32  # grid for training
  Nz_tr: 32  # grid for training
  num_epochs: 1000 # number of epochs to train
  print_rate: 10 # frequency of printing epochs and loss during training
  restart_from_checkpoint: false # whether to restart from an existing checkpoint
  algorithm: 0  # 0: regression normal derivatives, 1: neural network normal derivatives
  switching_interval: 3  # domain switching interval
  version: 2  # 0: verbose/slow, 1: silent/fast, 2: refactored
  multi_gpu: false
  num_gpus_ddp: 1
  optim:
    optimizer_name: "custom" # options are "custom", "adam", "rmsprop"
    learning_rate: 1e-2
  sched:
    scheduler_name: "exponential" # options are "exponential", "polynomial"
    decay_rate: 0.96
  data_manager:
    refine: true
    refine_normals: false # if true will add extra points by moving grid points iteratively along minus normal of level-set function
    refine_lod: true # if true will add extra points by Octree using Kaolin

experiment:
  sphere:
    enable: True
    protein_name: single.pqr
    protein_path: inputs/sphere

  double_sphere:
    enable: False
    protein_name: double.pqr
    protein_path: inputs/sphere
  
  molecule:
    enable: False
    proteins_dir: inputs/pqr_input_mols
    num_gpus_batching: 3

  logging:
    checkpoint_interval: 500
    log_dir: ${hydra:runtime.output_dir}

