expname: benchmark-lpbe-app

defaults:
  - base_hydra_config
  - solver: poisson
  - _self_

# customization follows
gridstates:
  Nx_lvl: 128  # grid for level-set; not used in this example bc not free boundary
  Ny_lvl: 128  # grid for level-set; not used with level set is given functionally
  Nz_lvl: 128  # grid for level-set; not used with level set is given functionally
  Nx_eval: 256  # grid for evaluation/visualization/validation
  Ny_eval: 256  # grid for evaluation/visualization/validation
  Nz_eval: 256  # grid for evaluation/visualization/validation

model:
  model_type : "multiresolution_hash_network"
  mlp:
    hidden_layers_m: 1
    hidden_dim_m: 3
    activation_m: "jnp.tanh"
    hidden_layers_p: 2
    hidden_dim_p: 10
    activation_p: "jnp.tanh"
  resnet:
    res_blocks_m : 3
    res_dim_m : 40
    activation_m : "nn.tanh"
    res_blocks_p : 3
    res_dim_p : 80
    activation_p : "nn.tanh"
  discrete:
    interpolant: "trilinear" # "single_trilinear" or "trilinear" or "quadratic"
    Nx: 32
    Ny: 32
    Nz: 32
    xmin: 0.0
    xmax: 0.0
    ymin: 0.0
    ymax: 0.0
    zmin: 0.0
    zmax: 0.0
  multiresolution_hash_network:
    bound: 1.0
    pos_enc: "hashgrid"  # choices: "hashgrid" or "frequency" or "identity" or "spherical_harmonics"
    tv_scale: 0.0  # total variation
    pos_levels: 16  # encoding levels
    layer_widths: [64]  # layer widths, these are relu activated
    sol_skip_in_layers: []  # skip connections
    sol_act: 'linear'  # activations
    highest_sh_order: 8  # if using spherical_harmonics this is the highest sh order for hashgrid features


solver:
  inn_grid: false
  inn:
    train_out: 2000 # number of grid points in the outer domain; originally 2000
    train_inner: 100 # number of grid points in the inner domain; originally 100
    train_boundary: 1000 # number of grid points on the boundary; originally 1000
    train_gamma: 200 # number of grid points on the interface; originally 200
    Nx_tr: 2048  # grid for training voxel size: Lx/Nx
    Ny_tr: 2048  # grid for training voxel size: Ly/Ny
    Nz_tr: 2048  # grid for training voxel size: Lz/Nz
  uniform:
    Nx_tr: 32  # grid for training voxel size: Lx/Nx
    Ny_tr: 32  # grid for training voxel size: Ly/Ny
    Nz_tr: 32  # grid for training voxel size: Lz/Nz
  num_epochs: 10000 # number of epochs to train
  batch_size: 10000 # batch size for trainer. 
  print_rate: 100 # frequency of printing epochs and loss during training
  restart_from_checkpoint: false # whether to restart from an existing checkpoint
  restart_checkpoint_dir: /workspace/results/benchmark-lpbe-app-inn-1:1-2:10-Ngamma1000-epoch50k/checkpoints
  algorithm: 0  # 0: regression normal derivatives, 1: neural network normal derivatives
  mgrad_over_pgrad_scalefactor: 1.0  # a scaling factor to amplify minus gradients
  multi_gpu: false
  num_gpus_ddp: 1
  optim:
    optimizer_name: "rmsprop" # options are "custom", "adam", "rmsprop", "lbfgs"
    learning_rate: 1e-2
    sched:
      scheduler_name: "exponential" # options are "exponential", "polynomial"
      decay_rate: 0.9
  data_manager:
    refine: false
    refine_normals: false # if true will add extra points by moving grid points iteratively along minus normal of level-set function
    refine_lod: false # if true will add extra points by Octree using Kaolin
  # multigrid_cycling:
  #   v_cycle_period: 4
  #   rest_at_level: 50


experiment:
  sphere:
    enable: True
    protein_name: single.pqr
    protein_path: inputs/sphere

  double_sphere:
    enable: False
    protein_name: double.pqr
    protein_path: inputs/sphere
  
  molecule:
    enable: False
    proteins_dir: inputs/pqr_input_mols
    num_gpus_batching: 3

  logging:
    checkpoint_interval: 500
    log_dir: ${hydra:runtime.output_dir}

